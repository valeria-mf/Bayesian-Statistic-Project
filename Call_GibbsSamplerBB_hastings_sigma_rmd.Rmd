---
title: "Call_GibbsSamplerBB_hastings_sigma"
author: "Valeria Iapaolo"
date: "2024-01-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("Rcpp")
#install.packages("RcppEigen")

```

```{r}
library(Rcpp)
library(RcppEigen)
library(MASS)
library(ggplot2)
library(reshape2)
library(cowplot)
```

```{r}

set.seed(1234)
#Questo è il path del file header contenuto nella cartella unzippata dell'installazione di Eigen
Sys.setenv("PKG_CXXFLAGS"="-I\"C:\\Users\\ipval\\Desktop\\Progetto Bayesiana\\eigen-3.4.0\\eigen-3.4.0\"")

#Questo è il path del file creato con il comando install.packages("RcppEigen")
Sys.setenv(PKG_CXXFLAGS=paste0("-I", shQuote("C:/Users/ipval/AppData/Local/R/win-library/4.2/RcppEigen/include")))

Rcpp::sourceCpp("C:/Users/ipval/Desktop/ProgettoBayesiana/GibbsSampler_hastings_sigma.cpp")

```

```{r}

plot_dual_graphs <- function(n_iter, initial_iters, result) {
  par(mfrow=c(2,2))
  # Grafico di K
  plot(1:(n_iter + initial_iters), result$K_vector, 
       main = 'Number of latent features', xlab = 'Iteration', ylab = 'K+', type = 'l')
  abline(v = initial_iters, col = 'red')
  
  # Grafico log P(X,Z)
  plot(1:(n_iter + initial_iters), result$logPXZ_vector, 
       main = 'log P(X, Z)', xlab = 'Iteration', ylab = 'log P(X, Z)', type = 'l')
  abline(v = initial_iters, col = 'red')
  
  # Grafico sigma_A
  plot(1:(n_iter + initial_iters), result$sigmaA_vector, 
       main = 'sigma A', xlab = 'Iteration', ylab = 'sigma A', type = 'l')
  abline(v = initial_iters, col = 'red')
  
  # Grafico sigma_X
  plot(1:(n_iter + initial_iters), result$sigmaX_vector, 
       main = 'sigma X', xlab = 'Iteration', ylab = 'sigma X', type = 'l')
  abline(v = initial_iters, col = 'red')
}



plot_dual_heatmaps <- function(A_, Expected_A) {
  
 
  df_expected_a1 <- melt(A_)
  df_expected_a2 <- melt(Expected_A)
  
  
  plot1 <- ggplot(df_expected_a1, aes(x = Var2, y = Var1, fill = value)) +
    geom_tile() +
    labs(title = "Original A",
         x = "Column Index",
         y = "Row Index") +
    scale_fill_gradient(low = "white", high = "blue", name = "Value") +
    theme_minimal()
  
  plot2 <- ggplot(df_expected_a2, aes(x = Var2, y = Var1, fill = value)) +
    geom_tile() +
    labs(title = "Expected A",
         x = "Column Index",
         y = "Row Index") +
    scale_fill_gradient(low = "white", high = "blue", name = "Value") +
    theme_minimal()
  
  combined_plot <- plot_grid(plot1, plot2,  ncol = 2)
  
  print(combined_plot)
}





plot_combined_graphs <- function(n_iter, initial_iters, result) {
  # Creazione del dataframe per i dati
  data <- data.frame(iteration = 1:(n_iter + initial_iters),
                     K = result$K_vector,
                     logPXZ = result$logPXZ_vector)
  
  # Creazione dei grafici
  plot_k <- ggplot(data, aes(x = iteration, y = K)) +
    geom_line() +
    labs(title = 'Number of latent features',
         x = 'Iteration',
         y = 'K+') +
    theme_minimal() +
    geom_vline(xintercept = initial_iters, linetype = 'dashed', color = 'red')
  
  plot_logPXZ <- ggplot(data, aes(x = iteration, y = logPXZ)) +
    geom_line() +
    labs(title = 'log P(X, Z)',
         x = 'Iteration',
         y = 'log P(X, Z)') +
    theme_minimal() +
    geom_vline(xintercept = initial_iters, linetype = 'dashed', color = 'red')
  
  # Combina i due grafici sopra e sotto
  combined_plot <- plot_grid(plot_k, plot_logPXZ, nrow = 2)
  
  # Visualizza il grafico combinato
  print(combined_plot)
}


```


Definiamo ora una funzione per generare la matrice Z che rispetti il modello Beta-Bernoulli. Creiamo una matrice Z con dimensione N x n_tilde e poi rimuoviamo le colonne nulle ottenendo una matrice N x K, dove K indica il numero di features
```{r}

generateRandomMatrix <- function(N, alpha, theta, n_tilde) {
  m=rep(0,n_tilde)
  Z=matrix(0, nrow=N, ncol=n_tilde) 
  for(i in 1:N){
    #Estraggo tra le features già osservate
    features_osservate=(m!=0)
    k=sum(m!=0)
    if(k!=0){
      for(j in which(features_osservate)){
        Z[i,j]=rbinom(1,1,(m[j]-alpha)/(theta+i-1))
      }
      

    }
    #Estraggo le nuove features
    nuove_features=rbinom(1,n_tilde-k,1-(theta+alpha+i-1)/(theta+i-1))
    if(nuove_features!=0){
      if(nuove_features==1){
        Z[i,k+1]=1
      }
      else{
        range=(k+1):(k+nuove_features)
        Z[i,range]=1
        
      }
     
    }
    
    #Aggiorno il vettore m
    m=colSums(Z)
  }
    
    
  return(Z)
}
```


Modifico proposal_variance_factor_sigma_x e proposal_variance_factor_sigma_a in base all'acceptance probability per sigma_x e sigma_a (queste probabilità devono avere valori in [0.3; 0.5])

```{r}
#Genero Z
alpha=-1
theta=4
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100
a_x= 1
b_x= 1
a_a=1
b_a=1
proposal_variance_factor_sigma_x=0.01
proposal_variance_factor_sigma_a=1


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_betabernoulli(alpha, theta, sigma_x, sigma_a, a_x, b_x, a_a, b_a, 
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters, proposal_variance_factor_sigma_x,
                                     proposal_variance_factor_sigma_a)
result$K_vector

print(paste("Acceptance probability for sigma_a:", result$acceptance_probability_a))
print(paste("Acceptance probability for sigma_x:", result$acceptance_probability_x))
```

```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)

```

```{r}
#Genero Z
alpha=-0.1
theta=0.5
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100
a_x= 1
b_x= 1
a_a=1
b_a=1
proposal_variance_factor_sigma_x=0.01
proposal_variance_factor_sigma_a=1


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_betabernoulli(alpha, theta, sigma_x, sigma_a, a_x, b_x, a_a, b_a, 
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters, proposal_variance_factor_sigma_x,
                                     proposal_variance_factor_sigma_a)
result$K_vector

print(paste("Acceptance probability for sigma_a:", result$acceptance_probability_a))
print(paste("Acceptance probability for sigma_x:", result$acceptance_probability_x))
```

```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```

```{r}
#Genero Z
alpha=-10
theta=12
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100
a_x= 1
b_x= 1
a_a=1
b_a=1
proposal_variance_factor_sigma_x=0.01
proposal_variance_factor_sigma_a=1


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_betabernoulli(alpha, theta, sigma_x, sigma_a, a_x, b_x, a_a, b_a, 
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters, proposal_variance_factor_sigma_x,
                                     proposal_variance_factor_sigma_a)
result$K_vector

print(paste("Acceptance probability for sigma_a:", result$acceptance_probability_a))
print(paste("Acceptance probability for sigma_x:", result$acceptance_probability_x))
```


```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```

