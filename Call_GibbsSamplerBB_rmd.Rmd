---
title: "Call_GibbsSamplerBB"
author: "Valeria Iapaolo"
date: "2024-01-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("Rcpp")
#install.packages("RcppEigen")

```

```{r}
library(Rcpp)
library(RcppEigen)
library(MASS)
library(ggplot2)
library(reshape2)
library(cowplot)
```

```{r}

set.seed(1234)
#Questo è il path del file header contenuto nella cartella unzippata dell'installazione di Eigen
Sys.setenv("PKG_CXXFLAGS"="-I\"C:\\Users\\ipval\\Desktop\\Progetto Bayesiana\\eigen-3.4.0\\eigen-3.4.0\"")

#Questo è il path del file creato con il comando install.packages("RcppEigen")
Sys.setenv(PKG_CXXFLAGS=paste0("-I", shQuote("C:/Users/ipval/AppData/Local/R/win-library/4.2/RcppEigen/include")))

Rcpp::sourceCpp("C:/Users/ipval/Desktop/ProgettoBayesiana/GibbsSampler_betabernoulli.cpp")

```

```{r}

plot_dual_graphs <- function(n_iter, initial_iters, result) {
  # Grafico di K
  plot(1:(n_iter + initial_iters), result$K_vector, 
       main = 'Number of latent features', xlab = 'Iteration', ylab = 'K+', type = 'l')
  abline(v = initial_iters, col = 'red')
  
  # Grafico log P(X,Z)
  plot(1:(n_iter + initial_iters), result$logPXZ_vector, 
       main = 'log P(X, Z)', xlab = 'Iteration', ylab = 'log P(X, Z)', type = 'l')
  abline(v = initial_iters, col = 'red')
}



plot_dual_heatmaps <- function(A_, Expected_A) {
  
 
  df_expected_a1 <- melt(A_)
  df_expected_a2 <- melt(Expected_A)
  
  
  plot1 <- ggplot(df_expected_a1, aes(x = Var2, y = Var1, fill = value)) +
    geom_tile() +
    labs(title = "Original A",
         x = "Column Index",
         y = "Row Index") +
    scale_fill_gradient(low = "white", high = "blue", name = "Value") +
    theme_minimal()
  
  plot2 <- ggplot(df_expected_a2, aes(x = Var2, y = Var1, fill = value)) +
    geom_tile() +
    labs(title = "Expected A",
         x = "Column Index",
         y = "Row Index") +
    scale_fill_gradient(low = "white", high = "blue", name = "Value") +
    theme_minimal()
  
  combined_plot <- plot_grid(plot1, plot2,  ncol = 2)
  
  print(combined_plot)
}





plot_combined_graphs <- function(n_iter, initial_iters, result) {
  # Creazione del dataframe per i dati
  data <- data.frame(iteration = 1:(n_iter + initial_iters),
                     K = result$K_vector,
                     logPXZ = result$logPXZ_vector)
  
  # Creazione dei grafici
  plot_k <- ggplot(data, aes(x = iteration, y = K)) +
    geom_line() +
    labs(title = 'Number of latent features',
         x = 'Iteration',
         y = 'K+') +
    theme_minimal() +
    geom_vline(xintercept = initial_iters, linetype = 'dashed', color = 'red')
  
  plot_logPXZ <- ggplot(data, aes(x = iteration, y = logPXZ)) +
    geom_line() +
    labs(title = 'log P(X, Z)',
         x = 'Iteration',
         y = 'log P(X, Z)') +
    theme_minimal() +
    geom_vline(xintercept = initial_iters, linetype = 'dashed', color = 'red')
  
  # Combina i due grafici sopra e sotto
  combined_plot <- plot_grid(plot_k, plot_logPXZ, nrow = 2)
  
  # Visualizza il grafico combinato
  print(combined_plot)
}


```

Definiamo ora una funzione per generare la matrice Z che rispetti il modello Beta-Bernoulli. Creiamo una matrice Z con dimensione N x n_tilde e poi rimuoviamo le colonne nulle ottenendo una matrice N x K, dove K indica il numero di features

```{r}
generateRandomMatrix <- function(N, alpha, theta, n_tilde) {
  m=rep(0,n_tilde)
  Z=matrix(0, nrow=N, ncol=n_tilde) 
  for(i in 1:N){
    #Estraggo tra le features già osservate
    features_osservate=(m!=0)
    k=sum(m!=0)
    if(k!=0){
      for(j in which(features_osservate)){
        Z[i,j]=rbinom(1,1,(m[j]-alpha)/(theta+i-1))
      }
      

    }
    #Estraggo le nuove features
    nuove_features=rbinom(1,n_tilde-k,1-(theta+alpha+i-1)/(theta+i-1))
    if(nuove_features!=0){
      if(nuove_features==1){
        Z[i,k+1]=1
      }
      else{
        range=(k+1):(k+nuove_features)
        Z[i,range]=1
        
      }
     
    }
    
    #Aggiorno il vettore m
    m=colSums(Z)
  }
    
    
  return(Z)
}
```

Facciamo una prova:
```{r}
generateRandomMatrix(4,-1,4,10)
```
Caso BANALE: genero i dati secondo il modello beta bernouilli e passo al Gibbs Sampler direttamente la matrice A giusta (questa non verrà modificata dal Gibbs Sampler) e inizializzo K, il numero di features con n_tilde

```{r}
#Genero Z
alpha=-1
theta=4
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_betabernoulli(alpha, theta, sigma_x, sigma_a,
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters)
result$K_vector

```

```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)

```

Notiamo che l'algoritmo capisce subito il numero corretto di features.

Ora, invece, utilizzando gli stessi parametri della simulazione precedente passiamo all'algoritmo una matrice A diversa da quella utilizzata per generare i dati: passiamoo una matrice A con elementi generati da una normale con media 0e varianza 1.
```{r}
alpha=-1
theta=4
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero



D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z
A_da_passare=matrix(rnorm(K*D,0,1), nrow = K, ncol = D)

result <- GibbsSampler_betabernoulli(alpha, theta, sigma_x, sigma_a,
                                     n_tilde, n=N, A_da_passare, X_, n_iter, initial_iters)
result$K_vector
```

Otteniamo lo stesso risultato (FORSE perchè la matrice A non ha nessuna influenza nell'algoritmo)

Proviamo ora a modificare anche il valore di sigma_a e sigma_x. Non passo più il valore reale ma, ad esempio, 1.
```{r}
#Genero Z
alpha=-1
theta=4
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_betabernoulli(alpha, theta, 1, 1,
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters)
result$K_vector

```

```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```
Anche in questo caso, cambiando i valorei di sigma_a e sigma_x, l'algoritmoriesce sin dalla prima iterazione a individuare il numero giusto di feature latenti.

Provo ora a variare il valore di alpha e theta rispettando il vincolo alpha < 0 e theta > -alpha.
```{r}
#Genero Z
alpha=-0.1
theta=0.5
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_betabernoulli(alpha, theta, sigma_x, sigma_a,
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters)
result$K_vector
```


```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```

```{r}
#Genero Z
alpha=-10
theta=12
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_betabernoulli(alpha, theta, sigma_x, sigma_a,
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters)
result$K_vector
```
```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```


Osservazione: il numero di nuove features viene estratto da una binomiale con parametri n_tilde-k (dove k è il numero di feature osservate fino a quel momento) e -alpha/(theta+n).
All'aumentare del modulo di alpha aumenta K.
All'aumentare di theta diminuisce K.


In tutti i casi visti K viene identificato correttamente e sin dalla prima iterazione.



Provo ora ad aggiungere alla matrice X del rumore (gaussiana standard)
```{r}
#Genero Z
alpha=-1
theta=4
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


#Aggiugo del rumore alla matrice X_
X_ <- matrix(rnorm(K*D, 0, 1), nrow = K, ncol = D)

result <- GibbsSampler_betabernoulli(alpha, theta, 1, 1,
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters)
result$K_vector
```

```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```

Provo adaumentare la varianza del rumore pervedere cosa succede
```{r}
#Genero Z
alpha=-1
theta=4
N=21
n_tilde=24
Z_true= generateRandomMatrix(N, alpha, theta, n_tilde)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
sigma_x=2
sigma_a=5
mean_a=0
n_iter=1000
initial_iters=100


A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


#Aggiugo del rumore alla matrice X_
X_ <- matrix(rnorm(K*D, 0, 3), nrow = K, ncol = D)

result <- GibbsSampler_betabernoulli(alpha, theta, 1, 1,
                                     n_tilde, n=N, A_, X_, n_iter, initial_iters)
result$K_vector


```




```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```