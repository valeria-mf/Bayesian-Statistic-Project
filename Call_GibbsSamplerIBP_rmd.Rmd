---
title: "Call_GibbsSamplerIBP"
author: "Valeria Iapaolo"
date: "2024-01-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("Rcpp")
#install.packages("RcppEigen")

```

```{r}
library(Rcpp)
library(RcppEigen)
library(MASS)
library(ggplot2)
library(reshape2)
library(cowplot)
```

```{r}

set.seed(1234)
#Questo è il path del file header contenuto nella cartella unzippata dell'installazione di Eigen
Sys.setenv("PKG_CXXFLAGS"="-I\"C:\\Users\\ipval\\Desktop\\Progetto Bayesiana\\eigen-3.4.0\\eigen-3.4.0\"")

#Questo è il path del file creato con il comando install.packages("RcppEigen")
Sys.setenv(PKG_CXXFLAGS=paste0("-I", shQuote("C:/Users/ipval/AppData/Local/R/win-library/4.2/RcppEigen/include")))

Rcpp::sourceCpp("C:/Users/ipval/Desktop/ProgettoBayesiana/GibbsSamplerIBP.cpp")

```

```{r}

plot_dual_graphs <- function(n_iter, initial_iters, result) {
  # Grafico di K
  plot(1:(n_iter + initial_iters), result$K_vector, 
       main = 'Number of latent features', xlab = 'Iteration', ylab = 'K+', type = 'l')
  abline(v = initial_iters, col = 'red')
  
  # Grafico log P(X,Z)
  plot(1:(n_iter + initial_iters), result$logPXZ_vector, 
       main = 'log P(X, Z)', xlab = 'Iteration', ylab = 'log P(X, Z)', type = 'l')
  abline(v = initial_iters, col = 'red')
}



plot_dual_heatmaps <- function(A_, Expected_A) {
  
 
  df_expected_a1 <- melt(A_)
  df_expected_a2 <- melt(Expected_A)
  
  
  plot1 <- ggplot(df_expected_a1, aes(x = Var2, y = Var1, fill = value)) +
    geom_tile() +
    labs(title = "Original A",
         x = "Column Index",
         y = "Row Index") +
    scale_fill_gradient(low = "white", high = "blue", name = "Value") +
    theme_minimal()
  
  plot2 <- ggplot(df_expected_a2, aes(x = Var2, y = Var1, fill = value)) +
    geom_tile() +
    labs(title = "Expected A",
         x = "Column Index",
         y = "Row Index") +
    scale_fill_gradient(low = "white", high = "blue", name = "Value") +
    theme_minimal()
  
  combined_plot <- plot_grid(plot1, plot2,  ncol = 2)
  
  print(combined_plot)
}





plot_combined_graphs <- function(n_iter, initial_iters, result) {
  # Creazione del dataframe per i dati
  data <- data.frame(iteration = 1:(n_iter + initial_iters),
                     K = result$K_vector,
                     logPXZ = result$logPXZ_vector)
  
  # Creazione dei grafici
  plot_k <- ggplot(data, aes(x = iteration, y = K)) +
    geom_line() +
    labs(title = 'Number of latent features',
         x = 'Iteration',
         y = 'K+') +
    theme_minimal() +
    geom_vline(xintercept = initial_iters, linetype = 'dashed', color = 'red')
  
  plot_logPXZ <- ggplot(data, aes(x = iteration, y = logPXZ)) +
    geom_line() +
    labs(title = 'log P(X, Z)',
         x = 'Iteration',
         y = 'log P(X, Z)') +
    theme_minimal() +
    geom_vline(xintercept = initial_iters, linetype = 'dashed', color = 'red')
  
  # Combina i due grafici sopra e sotto
  combined_plot <- plot_grid(plot_k, plot_logPXZ, nrow = 2)
  
  # Visualizza il grafico combinato
  print(combined_plot)
}


```


Definiamo ora una funzione per generare la matrice Z che rispetti l'Indian Buffet Process. Creiamo una matrice Z con dimensione N x numero_colonne e poi rimuoviamo le colonne nulle ottenendo una matrice N x K, dove K indica il numero di features

```{r}
generateRandomMatrix <- function(N, alpha, theta, gamma, numero_colonne) {
  m=rep(0,numero_colonne)
  Z=matrix(0, nrow=N, ncol=numero_colonne) 
  for(i in 1:N){
    #Estraggo tra le features già osservate
    features_osservate=(m!=0)
    k=sum(m!=0)
    if(k!=0){
      for(j in which(features_osservate)){
        Z[i,j]=rbinom(1,1,(m[j]-alpha)/(theta+i-1))
      }
      

    }
    #Estraggo le nuove features
    nuove_features=rpois(1, gamma*gamma(theta+alpha+i-1)*gamma(theta+1)/gamma(theta+alpha)/gamma(theta+1+i-1))
    if(nuove_features!=0){
      if(nuove_features==1){
        Z[i,k+1]=1
      }
      else{
        range=(k+1):(k+nuove_features)
        Z[i,range]=1
        
      }
     
    }
    
    #Aggiorno il vettore m
    m=colSums(Z)
  }
    
    
  return(Z)
}
```

Facciamo una prova rispettando i vincoli gamma>=0, 0<=alpha<=1 e theta>=-alpha.

```{r}
generateRandomMatrix(4, 0.1, 1.2, 1.2, 10)
```


```{r}
#Genero Z
alpha=0.1
gamma=1.2
theta=0.3
N=21
numero_colonne=100   #questo valore deve essere sufficientemente grande
Z_true= generateRandomMatrix(N, alpha, theta, gamma, numero_colonne)
K=sum(colSums(Z_true) != 0) #numero colonne diverse da 0 di Z
K
colonne_non_zero <- colSums(Z_true) != 0
Z_true <- Z_true[, colonne_non_zero]   #rimuovo le colonne pari a zero

#X e A
D=30
n_tilde=24
sigma_a=5
sigma_x=2
mean_a=0
n_iter=1000
initial_iters=100
UB=20



A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )
}

numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z


result <- GibbsSampler_IBP(alpha, gamma, sigma_a, sigma_x,  theta,  N, 
                           A_,  X_, UB, n_iter, initial_iters)
result$K_vector

```

```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)

```
```{r}
N=21
K=18 
D=30
sigma_x=2
sigma_a=5
mean_a=1
n_iter=1000
initial_iters=100

Z_true <- generateRandomMatrix(N, K)  
A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )

  
}
numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z

result <- GibbsSampler_betabernoulli(alpha=-7, theta=13, sigma_x=sigma_x, sigma_a=sigma_a,
                                     n_tilde=30, n=N, A_=A_, X_=X_, n_iter=n_iter, initial_iters=initial_iters)
result$K_vector

```

```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```

```{r}
N=21
K=24 
D=30
sigma_x=2
sigma_a=5
mean_a=1
n_iter=1000
initial_iters=100

Z_true <- generateRandomMatrix(N, K)  
A_ <- matrix(rnorm(K*D,mean_a,sigma_a), nrow = K, ncol = D)
X_ <- matrix(0, nrow=N, ncol=D)
for (i in 1:N){
  X_[i,] <- mvrnorm(1, mu = Z_true[i,]%*%A_ , Sigma = sigma_x^2*diag(D) )

  
}
numero_colonne_diverse_da_zero_di_Z <- sum(colSums(Z_true != 0) > 0)
numero_colonne_diverse_da_zero_di_Z

result <- GibbsSampler_betabernoulli(alpha=-7, theta=13, sigma_x=sigma_x, sigma_a=sigma_a,
                                     n_tilde=30, n=N, A_=A_, X_=X_, n_iter=n_iter, initial_iters=initial_iters)
result$K_vector
```


```{r}
plot_dual_graphs(n_iter, initial_iters, result)
plot_dual_heatmaps(A_, result$Expected_A)
```

